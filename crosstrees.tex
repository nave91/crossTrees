
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This  is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads]{llncs}

\usepackage{amssymb}
\usepackage{cite}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann,ursula.barth,ingrid.beyer,natalie.brecht,|
\urldef{\mailsb}\path|christine.guenther,frank.holzwarth,piamaria.karbach,|
\urldef{\mailsc}\path|anna.kramer,erika.siebert-cole,lncs}@springer.com|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\newenvironment{myepigraph}
  {\par\hfill\itshape
   \begin{tabular}{@{}r@{}}} % 2em from the right margin
  {\end{tabular}\par\medskip}


\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\tion}[1]{\S\ref{sec:#1}}


\usepackage[small,compact]{titlesec}



\usepackage{times}
\def\baselinestretch{0.95}


 \usepackage[font={small}]{caption, subfig}
 \renewcommand{\figurename}{Fig.}
 \renewcommand{\tablename}{Tab.}
\usepackage{enumitem}
\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}
 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\setlist{nosep}

%\usepackage{biblatex}
%\renewcommand*{\bibfont}{\footnotesize}
%\usepackage[sort&compress]{natbib}  
 \newcommand{\bibfont}{\tiny}
% \setlength{\bibsep}{0ex}
%\renewcommand\section{\@startsection{section}{1}{\z@}{-10\p@ \@plus -4\p@ \@minus -4\p@}{5\p@ \@plus 4\p@ \@minus 4\p@}{\normalfont\large\bfseries\boldmath\rightskip=\z@ \@plus 8em\pretolerance=10000 }}


\begin{document}


\mainmatter  % start of an individual contribution

% first the title is needed
\title{Finding Explanations for Multi-Objective Optimization
(in Near-Linear Time)}

% a short form should be given in case it is too long for the running head
\titlerunning{SSBSE'15}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Authors suppressed for blind review}

%<nalekkalapudi@mix.wvu.edu>Tim Menzies%
%%\and Ursula Barth\and Ingrid Beyer\and Natalie Brecht\and\\
%Christine G\"{u}nther\and Frank Holzwarth\and Pia Maria Karbach \and\\
%Anna Kramer\and Erika
%Siebert-Cole}
%
\authorrunning{Explaing MOEAs, SSBSE'15}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Institution suppressed for blind review}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%
 
\maketitle


\begin{abstract}
CT0 is an algorithm for summarizing trade-offs in multi-objective problems.
From that summary, humans can read recommendations for their systems.
This paper evaluates those recommendations using data
generated from   (a) the POM3 model of agile selection
of tasks; (b) four COCOMO-suite predictors for software
development effort, months, defects and risk.

CT0 is a very fast algorithm- both theoretically and empirically.
For example, for the COCOMO-suite models, CT0 terminated
in 3 seconds while standard optimizers (NSGA-II and SPEA2)
took 150 seconds.
Further, the  generated 
explanations for CT0 were just as effective as from other optimizers.

Hence, we recommend CT0 when some succinct summary
has to be rapidly generated (e.g.  in some
interactive design meeting). CT0 could also be
useful as post-processor to other optimizers (to
generate succinct explanations of their conclusions)
or as a optimizer to other optimizers (by
constraining those other optimizers to only search
the regions recommended by CT0).

\keywords{Software engineering, explanation, optimization, multi-objective.}
\end{abstract}



\section{Introduction}

\begin{myepigraph}
``If you cannot- in the long run- tell  everyone what \\
 you have been doing, your  doing has been worthless.''\\
 {\em-- Erwin Schr\"odinger}

\end{myepigraph}


Explaining the results of
multi-objective optimization to a user
can be problematic.
A typical run of a multi-objective optimizer
can process thousands to millions of examples.
It is an overwhelming task for humans to
certify the correctness of conclusions generated
from so many results. Verrappa and Leiter warn that
\begin{quote}
``..for industrial problems, these algorithms generate
(many) solutions, which makes the tasks of
understanding them and selecting one among them
difficult and time consuming''~\cite{veer11}.
\end{quote}
Even if explanations are constrained to
 (say) just a few hundred examples taken from the Pareto
frontier, this can still confuse the user.
 Valerdi notes that it can take days for
panels of human experts to rigorously review even a few dozen
examples~\cite{valerdi11}.  
For example, once had a 
client who disputed the results of our  analysis.
They
demanded to  audit the reasoning but
 when we delivered the 
of candidate solutions on the Pareto frontier,
they were overwhelmed by
the amount of information.  Flustered,
the client discounted the  analysis
and rejected our conclusions. From this experience, we learned that
to better support decision making in SBSE, we must better explain
SBSE results.

Other researchers have recognized the importance
of explanation.
It is known to be a key factor in selecting algorithms.
For example, in the field of machine learning,
``each time one of our favorite 
approaches has been applied in industry, each time the
comprehensibility of the results, though ill-defined, has
been a decisive factor of choice over an approach by pure
statistical means, or by neural networks.''~\cite{ag98}.
Analogous terms to explainability  in that community 
are ``comprehensibility'',
``interpretability''~\cite{maimon05} or ``understandability''~\cite{allahyari:user-oriented}.

In spite of the importance attributed to the subject,
explanation has not been extensively investigated in the context of SBSE.
One of the few papers that does is that of 
 Veerappa and Lieter~\cite{veerappa11} who
clustered examples from the Pareto
frontier (examples generated from a goal graph representation of requirements
for London ambulance services). In this approach,
``instead of having to inspect a large
number of individual solutions, (users) can look at a
much smaller number of groups of related solutions,
and focus their attention on the important
characteristics of the group rather than the
particularities of their individual solutions''~\cite{veerappa11}. 

XXXX after creating. still errors in comparisons

While an innovative and insightful study, there
are three open issues with that method:  (a)~the complexity
of clustering; (b)~erroneous conclusions could be generated from the users 
inspection of the clusters; (c)~introduced by users
incorrectly evaluation the value of generated recommendations.
 Veerappa and Lieter did not evaluate the effects
of the recommendations that could be generated by users browsing their
clusters. 
Also, their method could suffer from scalability issues since it
a post-processor
to a clustering algorithm (clustering is a slow process  requiring
 say, $O(N^2)$ comparisons
for the greedy agglomerate clustering algorithm used in that paper~\cite{koc11b}).

Accordingly, in this paper, when:
\be
\item Cluster using a near-linear time algorithm;
\item Better define the process by which recommendations are generated
      from clusters;
\item The generated recommendations are tested by generating more
examples from the model {\em after} the recommendations are imposed
as extra constraints on the model inputs.
\ee

\section{A Motivating Example}
\input{eg}

\section{Models}
\input{models}

\section{Code}
\input{algo}

\section{Results}
\input{results}

\section{Explaining ``Explanation''}
As a starting point in this exploration of explanation,
it is important to distinguish between the
(1)~problem of explaining the output of an
multi-objective optimizer (discussed in this paper);
from the more complex problem of (2)~explaining how
that output was generated.  To put that another way:
we seek to explain eggs, but not
the chicken.

Next, a definition of  ``explanation'' is required such that:
\be
\item An explanation system can be designed;
\item It is possible to distinguish a  ``good'' for a ``bad'' explanation.
\ee
In the SE literature,
the general consensus in
software engineering is that ``good'' explanations
are succinct explanations~\cite{ag98,dej13z,fenton99}.
On this score, MOEAs fare poorly since their output can be very verbose
(hundreds or more examples from the Pareto frontier).


Also, cognitive science theory argues that
there is more to ``explaining'' something that just
showing it succinctly. According to Kelly's personal
construct theory (PCT) humans explain things via
``constructs'' that distinguish sets of examples~\cite{kelly55}.
So, for Kelly, human explanations are not about
``things'' in isolation but rather the {\em
differences between groups of things}. In data mining, finding
differences between things is called {\em contrast set learning}~\cite{webb09}.



Other  cognitive science research studied the activities
humans do during explanation generation.
Leake~\cite{leake91}
lists a dozen different tasks that humans perform when 
``explaining'' some phenomena. Leake does not claim
that the following list is complete; just that
it demonstrates  a wide range of
goal-based purposes for explanation, including:
\be
\item
Connect event to expected/believed conditions.
\item
Connect event to previously unexpected conditions.
\item
Find predictors for anomalous situation.
\item
Find repair points for causes of an undesirable state.
\item
Clarify current situation to predict effects or choose response.
\item
Find controllable (blockable or achievable) causes.
\item
Find actorsâ€™ contributions to outcome.
\item
Find motivations for anomalous actions or decisions.
\item
Find a within-theory derivation.
\ee
That is, to Leake, explanation is akin to planning
where the ``explainer'' is showing some audience how to 
find or connect together information.
A system that supports such explanations makes it easier
to ``connect the dots''. In practice that means an explanation system
must:
\bi
\item Input a large set of axioms: e.g. examples, pieces of background knowledge;
\item Output a {\em reduced} set of axioms:
e.g. rules, model fragments,
or as done by Veerappa and Lieter~\cite{veerappa11}, a small number
of representative examples takes from centroids of clusters on the Pareto
frontier;
\item Such that, in the reduces space, it is simple and quick to generate
goal-based explanations including the nine kinds listed above.
\ei
\subsection{Decision Trees as ``Explanation Tools''}
Decision tree learning is a widely-used framework for data mining: given a single goal
(called the ``class''), find some attribute value that splits the data such
that the distribution of classes in each split has been simplified
(where the simplest distribution is one containing examples from only one class).
Decision tree learners then grow sub-trees by recursing on the data in  each split. 
Popular decision-tree learners include:
\bi
\item CART~\cite{breiman84,} which  minimizes the variance of continuous classes in each split; or
\item C4.5~\cite{quinlan92} which minimizes the information content of the discrete classes in each split.
\ei
One reason to prefer decision trees is that they can very fast to
execute. Each level of recursion
processes progressively less data. Also, the computation at each level of the recursion
may be just a few linear passes through the data, followed by an
sort of the attributes-- so nothing more than $O(Nlog(N))$ at each level~\cite{witten11}.

Another reason to prefer decision trees is that, as discussed below, they
can operationalize much of Leake's and Kelly's
cognitive models on explanation.  Decision tree learners do have the disadvantage in that,
as used in standard pratice, they only focus on one goal. The aim of this
paper is to present a novel extension to standard decision tree learning that
extends them  to multi-objective optimization.

Given the above discussion, it is easy to see why that is so since decision
tree learners can operationalize the above definitions of ``explanation''.

One reason for the popularity of decision tree learners 
Previously, work on
constrast learning for single goal SE problems
found that very succinct contrast sets could be generated as a post-processor
to decision tree learning~\cite{me00e}:
\bi
\item Building a decision tree to separate the different outcomes;
\item Identifying leaves containing desired outcome $X$ and undesired outcome $Y$;
\item Querying that tree to find branches $B_x$ and $B_y$ that lead to
$X,Y$.
\item Computing  $B_x - B_y$ which   selects/rejects for
desired/undesired outcomes.
\ei
In one spectacularly successful demonstration of this technique~\cite{me03c}, it was found decision trees with 6,000 nodes had much
superfluous information.
Specifically, when 
some branch point high in the tree
most separated  the classes, then all contrast
set learning had to do is report those branch decisions that selected
for branches leading to the better classes.
Using that approach, a contrast set learning
could report 
contrasts   with only one to four variables in each (and when
applied to test data, those constrast sets were
at pruning away all the undesired outcomes). Other studies
with other data sets~\cite{me07} 
confirmed the {\bf
the law of tiny constrasts}:  {\em 
the minimal constrast set between things is usually much smaller
than a complete description of those things.}


For simple goal classification, one way to operationalize Leake's
framework is using decision trees. Given leaves of that tree $\{X,Y,Z,etc\}$, 
then exists some branch $\{B_x,B_y,B_z,etc\}$ that connects the root to the leaves
as a conjunction of attribute/value pairs.
Given some
opinion about the value of the contents of each leaf $\{U_x,U_Y,U_Z, etc\}$,  
then the set difference $B_x - B_y$ is the contrast set of the differences
that can drive examples on $X$ over to $Y$.


``from here to there''.

an explanation does not generate some single
unique output. Rather, it inputs a set of axioms or examples and
outputs a reduced set of axioms or examples within which 
it faster and simpler to generate explanations
 
Current MOEA algorithms are ``instance-based methods'' that return
specific examples that perform ``best'' with respect to the multiple goals.
The number of examples generated in this way can be overwhelming.

If a user wants to learn general principles from those examples,
some secondary {\em explanation} process is required to group and generalize those
examples.
For example,  Veerappa and Lieter~\cite{veerappa11} clustering examples from the Pareto
frontier so users (at a minimum) need only browse the centroids of each clusters).



GAs flat vectors, not the trees explored by by ()say) Gouse et al.

Goals is performance just as good but explain better

One caveat before beginning: if the audience for the
results of optimization are not human beings, then
perhaps an explanation systems is not required. For
example, Petke, Harman, Langdon, \&
Weimer~\cite{petke14} use evolutionary methods to
rewrite code such that the new code executes
faster. The audience for the rewritten code is a
compiler. Such compilers do not argue or and ask
questions about the code they are given to process.
Hence, that rewrite system does not necessary need
an explanation system.  That said, a succinct and
useful description of the difference between passing
and failing runs of the rewrite system could be
useful when (e.g.) a human is trying to debug that
code rewrite system.


Yet another model of ``explanation'' not explored
here is the ``surprise modeling'' approach
recommended by 
Freitas~\cite{Freitas98onobjective}, Voinea\&Tulea~\cite{voinea07} and
others including Horvitz~\cite{horvitz05}. In that
approach, (a)~some background knowledge
(e.g. summaries of prior actions by users) is used
to determine ``normal'' behavior; (b)~users are only
presented results that deviated from normal
expectations. In analogous research,
Koegh~\cite{keogh05} argues that {\em time series
  discords} (infrequent sequential events in a times
series) are a useful way to summarize reports from
complex temporal streams. The premise of surprise
modeling and reporting discords is that ``rare
events need to be explored''.  
In non-temporal domains, time series discords becomes
{\em anomaly detection}~\cite{chandola09}. For example, in the SE domian,
Voinea and Telea report tools that can quickly highlight regions
of unusually active debugging (and such regions should be reviewed
by management)~\cite{voiena07} (see also the anomaly
detection work of Gruska et al.~\cite{gruska10}).

We do not dispute the
importance of exploring anomalous outliers.  On the other
hand, when forming policies for software projects,
we need treatments that are well supported by the
data. Hence, our contrast sets report changes in the
data that, in our data, were {\em frequently} seen to
lead to change. 

Also, time series discords and anomaly detection are reports
on some variables. Hence, they have a different goal to CT0 that
strives to report recommendations on how to change the system
so to remove some problem. 

Further, all the systems described
above~\cite{voinea07,horvitz05,keogh05,gruska10} are
either for unsupervised learning (where no
objectives are known) or for single objective
systems (where only one goal is known). CT0, on the
other hand, is more ambitious since it was designed
for multi-objective systems.

Another potential issue with CT0 is
correlation-vs-causation conflation. The issue here
is that contrast sets will be useless if they
report spurious correlations and not true causal
effects.  Proving that some effect is truly causal
is a non-trivial task.  The standard Hall criteria
for causal effects~\cite{paul13} is so strict that,
outside of highly controlled lab conditions, it
rarely accepts that any effect is causal.  Hence, in
software engineering, when researchers talk of
causality~\cite{couto14,zheng14z,huber09,bhat!icse12}
they use Granger's ``predictive causality'';
i.e. causality is the ability of predicting
values seen in the future from values seen in the
past.  Elsewhere, Granger causality has been adapted
to data mining by organizing cross-validations such
that the test sets contain data collected at a later
time than the training sets~\cite{me11f}.  In this
paper, we adapt Granger casualty to search-based
methods by testing recommendations learned from $M$
simulations on a subsequent round of $N$ new
simulations.  Those recommendations satisfy Granger
causality when the subsequent round of $N$
simulations are changed in a manner predicted by the
recommendations gleaned from the original $M$
simulations.


``Data farming'' is a technique used extensively by the
U.S. Military~\cite{meyer04}.
Data farming builds a ``landscape''
of output that can be analyzed for trends, anomalies, and insights in
multiple parameter dimensions.  
In  a recent review of search-based and data mining methids in SE,
we found numerous examples
of data farming~\cite{strickland03,Myrtveit,Shepperd01,pearce99,vanlamsweerde98integrating,chung00,me03j,heaven11,rodriguez11,jian09}.

In theory. Once a project manager can view their project on the landscape,
they can use this visualization to determine

We come to this work after attending a recent seminar at the US 
Department of Defence's Software Engineering Institute (SEI), Pittsburgh, USA.
That seminar reflected on how to best broadcast the lessons learned by SEI
to a very broad audience.

In the $21^{st}$ century, it is now impossible to manually browse very
large quantities of software project data.
For example, as of October 2012,
Mozilla Firefox had 800K reports on software projects.  While it is
now possible to automatically analyze such data with data miners, at
some stage a group of business users will have to convene to {\em
  interpret the results} (e.g., to decide if it is wise to deploy the
results as a defect reduction method within an organization).
These business  users are now demanding that data mining tools
be augmented with tools to support  business-level
interpretation of that data. For example,

at a recent panel on software analytics at ICSE'12,

industrial practitioners lamented the state of the art in data mining

and software engineering~\cite{menzies12a}. Panelists commented that

``prediction is all well and good, but what about decision

making?''. That is, these panelists are more interested in the interpretations

that follow the mining, rather than just  the mining.


\section{Related Work}

Sayyad

GALE




{\scriptsize
\bibliographystyle{abbrv}


%\let\oldbibliography\thebibliography
%\renewcommand{\thebibliography}[1]{\oldbibliography{#1}
%\setlength{\itemsep}{0pt}} %Reducing spacing in the bibliography.
\bibliography{refs}}
\end{document}
