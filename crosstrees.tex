
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This  is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads]{llncs}

\usepackage{colortbl}

\usepackage{textcomp}

%quarts
\usepackage{colortbl} % not sure if needed
\usepackage[table]{xcolor} % not sure if needed

%%%% needed %%%
\usepackage{picture}
\newcommand{\quart}[3]{\begin{picture}(100,6)%1
{\color{black}\put(#3,3){\circle*{4}}\put(#1,3){\line(1,0){#2}}}\end{picture}}

\renewcommand{\labelitemi}{$\bullet$}

\usepackage{amssymb}
\usepackage{cite}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{alltt}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann,ursula.barth,ingrid.beyer,natalie.brecht,|
\urldef{\mailsb}\path|christine.guenther,frank.holzwarth,piamaria.karbach,|
\urldef{\mailsc}\path|anna.kramer,erika.siebert-cole,lncs}@springer.com|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\newenvironment{myepigraph}
  {\par\hfill\itshape
   \begin{tabular}{@{}r@{}}} % 2em from the right margin
  {\end{tabular}\par\medskip}
\newcommand{\bull}{$bullet$}
\usepackage{enumitem}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\tion}[1]{\S\ref{sec:#1}}

\usepackage{wrapfig}
\usepackage[small,compact]{titlesec}



\usepackage{times}
\def\baselinestretch{0.95}


 \usepackage[font={small}]{caption, subfig}
 \renewcommand{\figurename}{Fig.}
 \renewcommand{\tablename}{Tab.}

\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}
 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\setlist{nosep}

%\usepackage{biblatex}
%\renewcommand*{\bibfont}{\footnotesize}
%\usepackage[sort&compress]{natbib}  
 \newcommand{\bibfont}{\tiny}
% \setlength{\bibsep}{0ex}
%\renewcommand\section{\@startsection{section}{1}{\z@}{-10\p@ \@plus -4\p@ \@minus -4\p@}{5\p@ \@plus 4\p@ \@minus 4\p@}{\normalfont\large\bfseries\boldmath\rightskip=\z@ \@plus 8em\pretolerance=10000 }}


\begin{document}
%

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Wickedly fast Explanation Generation
for Multi-Objective Optimization}

% a short form should be given in case it is too long for the running head
\titlerunning{SSBSE'15}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Authors suppressed for blind review}

%<nalekkalapudi@mix.wvu.edu>Tim Menzies%
%%\and Ursula Barth\and Ingrid Beyer\and Natalie Brecht\and\\
%Christine G\"{u}nther\and Frank Holzwarth\and Pia Maria Karbach \and\\
%Anna Kramer\and Erika
%Siebert-Cole}
%
\authorrunning{Explaing MOEAs, SSBSE'15}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Institution suppressed for blind review}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%
 
\maketitle


\begin{abstract}
WICKED is a near linear-time 
algorithm for summarizing trade-offs in multi-objective problems.
Humans can read that summary to find recommendations for their systems.
This paper evaluates those recommendations using data
 from   (a) the POM3 model of agile selection
of tasks; (b) the COCOMO-suite predictors for software
development effort, months, defects and risk.
 WICKED's recommendations 
 were found to be just as effective at improving objective
scores as the actions of standard  optimizers.


WICKED runs orders of magnitude faster than  standard optimizers. For example, for one of our larger models,
WICKET and NSGA-II terminated in 
3 and 150 minutes, respectively.
Hence, we recommend WICKED when a succinct summary
has to be rapidly generated (e.g.  in some
interactive design meeting). 
%WICKED could also be
%useful as post-processor to other optimizers (to
%generate succinct explanations of their conclusions)
%or as a way to speed up other optimizers (by
%constraining those  optimizers to  search
%the regions recommended by WICKED).

\keywords{Software engineering, explanation, optimization, multi-objective.}
\end{abstract}



\section{Introduction}

\begin{myepigraph}
``If you cannot- in the long run- tell  everyone what you have\\
been doing, your  doing has been worthless.'' {\em-- Erwin Schr\"odinger}
\end{myepigraph}
\noindent
To better support decision making in SBSE, we must better explain
SBSE results.
Explaining the results of these
multi-objective optimizers to a user
can be problematic.
A typical run of a multi-objective optimizer
can process thousands to millions of examples.
It is an overwhelming task for humans to
certify the correctness of conclusions generated
from so many results. Verrappa and Leiter warn that
\begin{quote}
``..for industrial problems, these algorithms generate
(many) solutions, which makes the tasks of
understanding them and selecting one among them
difficult and time consuming''~\cite{veerappa11}.
\end{quote}
Even if explanations are constrained to
 (say) just a few hundred examples taken from the Pareto
frontier, this can still confuse the user.
 Valerdi notes that it can take days for
panels of human experts to rigorously review even a few dozen
examples~\cite{valerdi11}.  
For example, once we had a 
client 
demanding to  audit our SBSE reasoner.
 When we delivered the 
of candidate solutions on the Pareto frontier,
they were overwhelmed by
the amount of information.  Flustered,
the client discounted our  analysis.


Other researchers have recognized the importance
of explanation.
For example, in the field of machine learning,
it is known to be a key factor in selecting algorithms.
``each time one of our favorite 
approaches has been applied in industry, each time the
comprehensibility of the results, though ill-defined, has
been a decisive factor of choice over an approach by pure
statistical means, or by neural networks.''~\cite{ag98}.

In spite of the importance of explanation,
there are few papers on this topic in the context of
SBSE.
One notable exception comes from
 Veerappa and Lieter~\cite{veerappa11} who
clustered examples from the Pareto
frontier (examples generated from a goal graph of requirements
for London ambulance services). In this approach,
``instead of having to inspect a large
number of individual solutions, (users) can look at a
much smaller number of groups of related solutions,
and focus their attention on the important
characteristics of the group rather than the
particularities of their individual solutions''~\cite{veerappa11}. 

The Veerappa and Lieter study is an interesting investigation
of an important issue that is rarely explored in the SBSE literature.
That said, is there a better approach? 
SBSE algorithms are slow enough
without the additional cost of some 
 $O(N^2)$ post-processing clustering.
Also, given that both clustering and SBSE algorithms divide and explore
examples, perhaps there is something we can do ``under the hood'' to unify SBSE and
clustering, thus removing the need for  extra architecture.

This paper introduces WICKED, an experiment with an  ``under the hood'' unification of SBSE
and a near-linear time clustering algorithm. 
WICKED has several parts:
\bi
\item \underline{{\bf W}}HERE4 is a linear-time clusterer that divides the population;
\item \underline{{\bf I}}NFOGAIN is a linear-time feature selector that  prunes irrelevancies;
\item  \underline{{\bf C}}ART is a  decision tree learning that find branches to different clusters;
\item \underline{{\bf K}}ILL is a branch pruner that simplifies (shortens) the branches;
\item \underline{{\bf E}}NVY is a contrast set learner that  find close pairs of {\em worse,better} clusters;
\item \underline{{\bf D}}ELTA generates recommendations from the difference of the branches between {\em worse, better} clusters.
\ei
The first steps (W.I.C.K.)
generate a very small
decision tree that divides up the multi-objective decision space in to many small regions.
After that, the last steps (E.D.) can be automatic or manual tasks performed by users
as they trace out effects over their multi-objective problems.
The rest of this paper addresses four research questions:

\bi
\item[{\bf RQ1}:] {\em Can WICKED generates \underline{succinct} summaries of multi-objective problems.} 
We show that, at least for the models explored here, the trees
generated by W.I.C.K. are very short (25 lines, or less).
\item[{\bf RQ2}:] {\em Are WICKED's summaries \underline{explanations} of multi-objective problems?}
We will defend this claim using theory taken from cognitive psychology.
\item[{\bf RQ3:}] {\em Are WICKED's  explanations \underline{effective}?}
We will re-run our models use WICKED's 
recommendations as constraints. This will  achieve optimizations comparable
to standard MOEA algorithms (NSGA-II).
\item [{\bf RQ4:}] {\em Does WICKED \underline{scale} to large problems?}
 We show that WICKED's
runtimes scale linearly with population size and terminates
faster than standard multi-objective optimizers
 (e.g. for one model, one minute for WICKED and 150 for NSGA-II~\cite{deb00a}).
\ei
Note that we do {\em not} claim WICKED is a
{\em better} at improving objective
scores than standard optimizers.  To make that case that WICKED is useful,
we need only show that WICKED's optimizations are no worse than standard methods,
while at the same time show that WICKED offers important additional services (explanation)
that scale to large problems.


\section{A Motivating Example}
\input{eg}



\section{Generating Explanations}
In the SE literature,
the general consensus in
software engineering is that ``good'' explanations
are succinct explanations~\cite{ag98,dej13z,fenton99}.
On this score, MOEAs fare poorly since their output can be very verbose
(hundreds or more examples from the Pareto frontier).

WICKED uses several techniques taken from cognitive science
and data mining in order to generate very succinct explanations.
For cognitive science, we use {\em constrast sets}.
According to Kelly's personal
construct theory (PCT) humans explain things via
``constructs'' that distinguish sets of examples~\cite{kelly55}.
So, for Kelly, human explanations are not about
``things'' in isolation but rather the {\em
differences between groups of things}. 

\begin{wrapfigure}{r}{2.3in}
\includegraphics[width=2.3in]{4WAY.png}
\caption{WHERE4. Each dot is
one set of decisions (inputs to a multi-objective model).
WHERE4 recurses on each of the right-hand-side clusters.
}\label{fig:4WAY}
\end{wrapfigure}
WICKED employees several techniques to operationalize
Kelly's theory.  Using {\em cluster and
  differentiate}, data is clustered and the
subsequent reasoning is restricting to only the
attributes that seperate those clusters. Next,
using only the attributes found by cluster and differentiate,
 {\em contrast set learning}~\cite{webb09} 
(a)~build rules to different clusters,
(b)~find pairs of clusters {\em worse, better}, then
(c)~report the delta between the branch to {\em worse} and
the branches to {\em better}.

\underline{{\em WHERE4: }}
WICKED's clusterer~\cite{me12d}
inputs a set of decisions
$d_1,d_2...$, each of which specifies
inputs to a multi-objective model.
It separate the decisions along an
approximate dimension
of greatest variability: a line joining two very dissimilar
decisions). This can be found in linear
time after just $2N$ comparisons,
using a technique proposed by Faloutsos and Lin~\cite{Faloutsos1995}: 
 pick any decision $d_i$ at random; find {\em East},
the most distance decision $d_j$; find {\em West}, the most distant 
distance $d_k$ to {\em East}\footnote{
 For distance, we use Aha's Euclidean measure~\cite{aha91};
i.e. $\sqrt{\sum_x(d_{ix}-d_{jx})^2}$ where $d_{ix},d_{jx}$ 
are decisions values normalized 0..1 for the range min..max.}.
Next, project all  decisions $d_i$ onto a two-dimensional
$(x,y)$ grid as follows:  
$a=\mathit{dist}(d_i,\mathit{West})$;
$b=\mathit{dist}(d_i,\mathit{East})$;
$c=\mathit{dist}(\mathit{East},\mathit{West})$;
$x=(a^2 + c^2 - b^2)/(2c)$;
$y=\sqrt{a^2 - x^2}$.
As illustrated in \fig{4WAY},
WHERE4 then clusters the decisions on the median $x,y$ values
and recurses on each cluster (division of $N$ examples terminates
when a cluster contains less than $\sqrt{N}$).

\underline{{\em INFOGAIN:}} To find attributes that differentiate between clusters,
WICKED uses the INFOGAIN entropy measure adapted from
Fayyad and Irani~\cite{FayIra93Multi}\footnote{For the reader
who knows that work, we comment that standard
FayyadIrani did not work for our models. We had to disable
the MDL pruning of sub-ranges since, for an optimizer to explore  small
and interesting ranges of simulation data, it is necessary
to let it ``take a chance'' on regions of the data that might not
be strongly supported by many examples.}. The values seen in $N$
decisions are sorted, then recursively split at points that
most simplifies the cluster distributions that use the values
in each part of the split. To measure simplicity, we use the
standard entropy measure. Let a split divides a decision's
values into two
lists  $v_1,v_2$ of size $n_1,n_2$. Let  the total counts
of cluster $c_k$ that use an element of $v_i$ be $c_{ik}$
and 
$p_{ik} = c_{ik}/n_i$. The entropy $e_i$ of the list $v_i$ can now be computed
as 
$-\sum_{k}p_{ik}log_2{p_{ik}}$ and the expected value of the simplicity
of a list after splitting is 
\mbox{$n_1/n*e_1 + n_2/n*e_2$} (where $n=n_1 + n_2$)
and {\em lower} values are {\em better} since they contain {\em fewer}
clusters.
Using this method, WICKED's INFOGAIN operator ranks 
all attributes according to the  simplicity of their associated
cluster distribution. It then rejects all but the simplest
25\% of the decisions and all subsequent reasoning in WICKED
uses just this small set of decisions that most distinguish
the cluster of decisions. Note that, like WHERE4, INFOGAIN is
also very fast to execute: after an initial top-level 
$O(nlog(n))$
sort of the decision values, the rest is just a
few linear
passes through the data.



\underline{{\em CART:}} Recall that in our
operationalization of Kelly's theory,
after {\em differentiate and
contrast} (with
WHERE4 and INFOGAIN), comes {\em constrast set learning}
(with CART,KILL,ENVY and DELTA).

Apart from succinctness, 
cognitive science theory argues that
there is more to ``explaining'' something that just
presenting some ideas very   succinctly. An important part of
an explanation system is that it presents something that
can be read in many different ways.
Leake~\cite{leake91}
lists a dozen different tasks that humans perform when 
``explaining'' some phenomena (Leake does not claim
that the following list is complete; just that
it demonstrates  a wide range of
goal-based purposes for explanation). Leake's list  includes:
\be
\item
Connect event to expected/believed conditions.
\item
Connect event to previously unexpected conditions.
\item
Find predictors for anomalous situations.
\item
Find repair points for causes of an undesirable state.
\item
Clarify current situation to predict effects or choose response.
\item
Find controllable (blockable or achievable) causes.
\item
Find actorsâ€™ contributions to outcome.
\item
Find motivations for anomalous actions or decisions.
\item
Find a within-theory derivation.  \ee 
Generalizing
from these examples, we say that a Leake-style
explanation system weaves together a set of axioms
into some story that overlaps, at least to some
degree, with the known facts or outcomes.  That
story will have gaps containing things that are
currently unknown but which might be assumed.

Working in the 1990s, Menzies formally characterized Leake's explanations as
the  {\em abductive} process where a theory $T$
is augmented with assumptions $A$ that (a)~do not lead to contridictions and
(b)~also lead to desired goals $G$\footnote{$T\cup A\vdash G$ and $T\cup A\vdash\not\bot$}.
Classic implementions of Leake-style explanation suffer from computational problems.



That is, to Leake, explanation is akin to planning
where the ``explainer'' is showing some audience how to 
find or connect together information.
A system that supports such explanations makes it easier
to ``connect the dots''. In practice that means an explanation system
must:
\bi
\item Input a large set of axioms: e.g. examples, pieces of background knowledge;
\item Output a {\em reduced} set of axioms:
e.g. rules, model fragments,
or as done by Veerappa and Lieter~\cite{veerappa11}, a small number
of representative examples takes from centroids of clusters on the Pareto
frontier;
\item Such that, in the reduces space, it is simple and quick to generate
goal-based explanations including the nine kinds listed above.
\ei
\subsection{Decision Trees as ``Explanation Tools''}
Decision tree learning is a widely-used framework for data mining: given a single goal
(called the ``class''), find some attribute value that splits the data such
that the distribution of classes in each split has been simplified
(where the simplest distribution is one containing examples from only one class).
Decision tree learners then grow sub-trees by recursing on the data in  each split. 
Popular decision-tree learners include:
\bi
\item CART~\cite{breiman84,} which  minimizes the variance of continuous classes in each split; or
\item C4.5~\cite{quinlan92} which minimizes the information content of the discrete classes in each split.
\ei
One reason to prefer decision trees is that they can very fast to
execute. Each level of recursion
processes progressively less data. Also, the computation at each level of the recursion
may be just a few linear passes through the data, followed by an
sort of the attributes-- so nothing more than $O(Nlog(N))$ at each level~\cite{witten11}.

Another reason to prefer decision trees is that, as discussed below, they
can operationalize much of Leake's and Kelly's
cognitive models on explanation.  Decision tree learners do have the disadvantage in that,
as used in standard pratice, they only focus on one goal. The aim of this
paper is to present a novel extension to standard decision tree learning that
extends them  to multi-objective optimization.

Given the above discussion, it is easy to see why that is so since decision
tree learners can operationalize the above definitions of ``explanation''.

One reason for the popularity of decision tree learners 
Previously, work on
constrast learning for single goal SE problems
found that very succinct contrast sets could be generated as a post-processor
to decision tree learning~\cite{me00e}:
\bi
\item Building a decision tree to separate the different outcomes;
\item Identifying leaves containing desired outcome $X$ and undesired outcome $Y$;
\item Querying that tree to find branches $B_x$ and $B_y$ that lead to
$X,Y$.
\item Computing  $B_x - B_y$ which   selects/rejects for
desired/undesired outcomes.
\ei
In one spectacularly successful demonstration of this technique~\cite{me03c}, it was found decision trees with 6,000 nodes had much
superfluous information.
Specifically, when 
some branch point high in the tree
most separated  the classes, then all contrast
set learning had to do is report those branch decisions that selected
for branches leading to the better classes.
Using that approach, a contrast set learning
could report 
contrasts   with only one to four variables in each (and when
applied to test data, those constrast sets were
at pruning away all the undesired outcomes). Other studies
with other data sets~\cite{me07} 
confirmed the {\bf
the law of tiny constrasts}:  {\em 
the minimal constrast set between things is usually much smaller
than a complete description of those things.}


For simple goal classification, one way to operationalize Leake's
framework is using decision trees. Given leaves of that tree $\{X,Y,Z,etc\}$, 
then exists some branch $\{B_x,B_y,B_z,etc\}$ that connects the root to the leaves
as a conjunction of attribute/value pairs.
Given some
opinion about the value of the contents of each leaf $\{U_x,U_Y,U_Z, etc\}$,  
then the set difference $B_x - B_y$ is the contrast set of the differences
that can drive examples on $X$ over to $Y$.


``from here to there''.

an explanation does not generate some single
unique output. Rather, it inputs a set of axioms or examples and
outputs a reduced set of axioms or examples within which 
it faster and simpler to generate explanations
 
Current MOEA algorithms are ``instance-based methods'' that return
specific examples that perform ``best'' with respect to the multiple goals.
The number of examples generated in this way can be overwhelming.

If a user wants to learn general principles from those examples,
some secondary {\em explanation} process is required to group and generalize those
examples.
For example,  Veerappa and Lieter~\cite{veerappa11} clustering examples from the Pareto
frontier so users (at a minimum) need only browse the centroids of each clusters).



GAs flat vectors, not the trees explored by by ()say) Gouse et al.

Goals is performance just as good but explain better

One caveat before beginning: if the audience for the
results of optimization are not human beings, then
perhaps an explanation systems is not required. For
example, Petke, Harman, Langdon, \&
Weimer~\cite{petke14} use evolutionary methods to
rewrite code such that the new code executes
faster. The audience for the rewritten code is a
compiler. Such compilers do not argue or and ask
questions about the code they are given to process.
Hence, that rewrite system does not necessary need
an explanation system.  That said, a succinct and
useful description of the difference between passing
and failing runs of the rewrite system could be
useful when (e.g.) a human is trying to debug that
code rewrite system.


Yet another model of ``explanation'' not explored
here is the ``surprise modeling'' approach
recommended by 
Freitas~\cite{Freitas98onobjective}, Voinea\&Tulea~\cite{voinea07} and
others including Horvitz~\cite{horvitz05}. In that
approach, (a)~some background knowledge
(e.g. summaries of prior actions by users) is used
to determine ``normal'' behavior; (b)~users are only
presented results that deviated from normal
expectations. In analogous research,
Koegh~\cite{keogh05} argues that {\em time series
  discords} (infrequent sequential events in a times
series) are a useful way to summarize reports from
complex temporal streams. The premise of surprise
modeling and reporting discords is that ``rare
events need to be explored''.  
In non-temporal domains, time series discords becomes
{\em anomaly detection}~\cite{chandola09}. For example, in the SE domian,
Voinea and Telea report tools that can quickly highlight regions
of unusually active debugging (and such regions should be reviewed
by management)~\cite{voiena07} (see also the anomaly
detection work of Gruska et al.~\cite{gruska10}).

We do not dispute the
importance of exploring anomalous outliers.  On the other
hand, when forming policies for software projects,
we need treatments that are well supported by the
data. Hence, our contrast sets report changes in the
data that, in our data, were {\em frequently} seen to
lead to change. 

Also, time series discords and anomaly detection are reports
on some variables. Hence, they have a different goal to WICKED that
strives to report recommendations on how to change the system
so to remove some problem. 

Further, all the systems described
above~\cite{voinea07,horvitz05,keogh05,gruska10} are
either for unsupervised learning (where no
objectives are known) or for single objective
systems (where only one goal is known). WICKED, on the
other hand, is more ambitious since it was designed
for multi-objective systems.

Another potential issue with WICKED is
correlation-vs-causation conflation. The issue here
is that contrast sets will be useless if they
report spurious correlations and not true causal
effects.  Proving that some effect is truly causal
is a non-trivial task.  The standard Hall criteria
for causal effects~\cite{paul13} is so strict that,
outside of highly controlled lab conditions, it
rarely accepts that any effect is causal.  Hence, in
software engineering, when researchers talk of
causality~\cite{couto14,zheng14z,huber09,bhat!icse12}
they use Granger's ``predictive causality'';
i.e. causality is the ability of predicting
values seen in the future from values seen in the
past.  Elsewhere, Granger causality has been adapted
to data mining by organizing cross-validations such
that the test sets contain data collected at a later
time than the training sets~\cite{me11f}.  In this
paper, we adapt Granger casualty to search-based
methods by testing recommendations learned from $M$
simulations on a subsequent round of $N$ new
simulations.  Those recommendations satisfy Granger
causality when the subsequent round of $N$
simulations are changed in a manner predicted by the
recommendations gleaned from the original $M$
simulations.


``Data farming'' is a technique used extensively by the
U.S. Military~\cite{meyer04}.
Data farming builds a ``landscape''
of output that can be analyzed for trends, anomalies, and insights in
multiple parameter dimensions.  
In  a recent review of search-based and data mining methids in SE,
we found numerous examples
of data farming~\cite{strickland03,Myrtveit,Shepperd01,pearce99,vanlamsweerde98integrating,chung00,me03j,heaven11,rodriguez11,jian09}.

In theory. Once a project manager can view their project on the landscape,
they can use this visualization to determine

We come to this work after attending a recent seminar at the US 
Department of Defence's Software Engineering Institute (SEI), Pittsburgh, USA.
That seminar reflected on how to best broadcast the lessons learned by SEI
to a very broad audience.

In the $21^{st}$ century, it is now impossible to manually browse very
large quantities of software project data.
For example, as of October 2012,
Mozilla Firefox had 800K reports on software projects.  While it is
now possible to automatically analyze such data with data miners, at
some stage a group of business users will have to convene to {\em
  interpret the results} (e.g., to decide if it is wise to deploy the
results as a defect reduction method within an organization).
These business  users are now demanding that data mining tools
be augmented with tools to support  business-level
interpretation of that data. For example,

at a recent panel on software analytics at ICSE'12,

industrial practitioners lamented the state of the art in data mining

and software engineering~\cite{menzies12a}. Panelists commented that

``prediction is all well and good, but what about decision

making?''. That is, these panelists are more interested in the interpretations

that follow the mining, rather than just  the mining.



\section{Models}
\input{models}

\section{Code}
\input{algo}

\section{Methods}
\input{methods}
\section{Results}
\input{results}


\section{Related Work}

Sayyad

GALE

WHERE4, fastmap, platt
PDDF = where4

\section{to do}

Menzies has used combination of WHERE+ENVY has been
used previously for finding context-specific rules
for single-objective reasoning (reducing defects or
software development effort~\cite{me12d}).  What is
new here is the addition of CART+CON as well as the
application to multiple-objective reasoning.

{\scriptsize
\bibliographystyle{abbrv}


%\let\oldbibliography\thebibliography
%\renewcommand{\thebibliography}[1]{\oldbibliography{#1}
%\setlength{\itemsep}{0pt}} %Reducing spacing in the bibliography.
\bibliography{refs}}
\end{document}
